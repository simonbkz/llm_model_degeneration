{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64b1b037",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64f375e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiny corpus\n",
    "text = (\"hello world. \\n\"\n",
    "        \"hello transformer. \\n\"\n",
    "        \"attention is all you need. \\n\"\n",
    "        \"hello attention. \\n\"\n",
    "        \"transformers generate text. \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160f71ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build vocab (characters)\n",
    "chars = sorted(list(set(text)))\n",
    "enum_chars = {ch:i for i, ch in enumerate(chars)} #to mimic transformers tokenizer, we will need to tokenize subwords later\n",
    "item_chars = {i:ch for ch,i in enum_chars.items()}\n",
    "#vocab size from the corpus\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2714aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(s):\n",
    "    return torch.tensor([enum_chars[c] for c in s], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0cbc4b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(ids):\n",
    "    return \"\".join([item_chars[i] for i in ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9aa236f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d1b83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model = 64, nhead = 8, num_layers = 2, dim_feedforward = 128, block_size = 128, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_embedding = nn.Embedding(block_size, d_model)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward,dropout=dropout,activation='gelu',batch_first=True)\n",
    "        self.tr = nn.TransformerEncoder(encoder_layer, num_layers = num_layers)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        B, T = idx.shape\n",
    "        if T > self.block_size:\n",
    "            idx = idx[:, -self.block_size:]\n",
    "            T = self.block_size\n",
    "\n",
    "        position_ids = torch.arange(T, device = idx.device).unsqueeze(0).expand(B,T)\n",
    "        x = self.token_embedding(idx) + self.positional_embedding(position_ids)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c65c0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d47a8d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835d291d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (your_env_name)",
   "language": "python",
   "name": "your_env_name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
