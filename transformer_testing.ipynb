{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64b1b037",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64f375e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiny corpus\n",
    "text = (\"hello world. \\n\"\n",
    "        \"hello transformer. \\n\"\n",
    "        \"attention is all you need. \\n\"\n",
    "        \"hello attention. \\n\"\n",
    "        \"transformers generate text. \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160f71ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build vocab (characters)\n",
    "chars = sorted(list(set(text)))\n",
    "enum_chars = {ch:i for i, ch in enumerate(chars)} #to mimic transformers tokenizer, we will need to tokenize subwords later\n",
    "item_chars = {i:ch for ch,i in enum_chars.items()}\n",
    "#vocab size from the corpus\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2714aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(s):\n",
    "    return torch.tensor([enum_chars[c] for c in s], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0cbc4b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(ids):\n",
    "    return \"\".join([item_chars[i] for i in ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9aa236f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f3d1b83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model = 64, nhead = 8, num_layers = 2, dim_feedforward = 128, block_size = 128, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_embedding = nn.Embedding(block_size, d_model)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward,dropout=dropout,activation='gelu',batch_first=True)\n",
    "        self.tr = nn.TransformerEncoder(encoder_layer, num_layers = num_layers)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        B, T = idx.shape\n",
    "        if T > self.block_size:\n",
    "            idx = idx[:, -self.block_size:]\n",
    "            T = self.block_size\n",
    "\n",
    "        position_ids = torch.arange(T, device = idx.device).unsqueeze(0).expand(B,T)\n",
    "        x = self.token_embedding(idx) + self.positional_embedding(position_ids)\n",
    "\n",
    "    # prevent looking ahead\n",
    "        attn_mask = torch.triu(torch.ones(T,T, device=idx.device),diagonal=1).bool()\n",
    "        x = self.tr(x, mask = attn_mask)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4d0d76fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(model, start_text, max_new_chars=200, temperature = 1.0, top_k = None, device = 'cpu'):\n",
    "    model.eval()\n",
    "    idx = encode(start_text).unsqueeze(0).to(device)\n",
    "\n",
    "    for _ in range(max_new_chars):\n",
    "        logits = model(idx)[:,-1,:] / max(temperature, 1e-6)\n",
    "        if top_k is not None:\n",
    "            v, ix = torch.topk(logits, k = top_k, dim = 1)\n",
    "            mask = torch.full_like(logits, float(\"-inf\"))\n",
    "            mask.scatter_(1, ix, v)\n",
    "            logits = mask\n",
    "\n",
    "        probs = F.softmax(logits, dim = 1)\n",
    "        next_id = torch.multinomial(probs, num_samples=1)\n",
    "        idx = torch.cat([idx, next], dim = 1)\n",
    "\n",
    "    return decode(idx.squeeze(0).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c65c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training loop \n",
    "def get_batch(data, batch_size = 32, block_size = 64, device = 'cpu'):\n",
    "    ix = torch.randint(0, len(data) - block_size -1, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix]).to(device)\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]).to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b3d06057",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    model = TinyGPT(vocab_size, nhead = 4, num_layers = 2, dim_feedforward = 128, block_size = 128).to(device)\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr = 3e-4)\n",
    "\n",
    "    steps = 1200\n",
    "    batch_size = 64\n",
    "    block_size = 64\n",
    "\n",
    "    model.train()\n",
    "    for step in range(1, steps + 1):\n",
    "        x, y = get_batch(input_data, batch_size, block_size, device = device)\n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits.reshape(-1, vocab_size), y.reshape(-1))\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        if step % 200 == 0:\n",
    "            ppl = math.exp(loss.item())\n",
    "            print(f\"step {step:4d} | loss {loss.item():.3f} | ppl {ppl:.2f}\")\n",
    "\n",
    "    #generate sample text\n",
    "    print(\"\\n--- Generated text ---\")\n",
    "    for prompt in [\"hello\", \"attention\", \"transform\"]:\n",
    "        out = generate(model, prompt, max_new_chars = 120, temperature = 0.9, top_k = 10, device = device)\n",
    "        print(f\"\\nPrompt: {prompt!r}\\n{out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3d47a8d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'randit'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m train()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     12\u001b[39m model.train()\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, steps + \u001b[32m1\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     x, y = get_batch(input_data, batch_size, block_size, device = device)\n\u001b[32m     15\u001b[39m     logits = model(x)\n\u001b[32m     16\u001b[39m     loss = F.cross_entropy(logits.reshape(-\u001b[32m1\u001b[39m, vocab_size), y.reshape(-\u001b[32m1\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mget_batch\u001b[39m\u001b[34m(data, batch_size, block_size, device)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_batch\u001b[39m(data, batch_size = \u001b[32m32\u001b[39m, block_size = \u001b[32m64\u001b[39m, device = \u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     ix = torch.randit(\u001b[32m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(data) - block_size -\u001b[32m1\u001b[39m, (batch_size,))\n\u001b[32m      4\u001b[39m     x = torch.stack([data[i:i+block_size] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m ix]).to(device)\n\u001b[32m      5\u001b[39m     y = torch.stack([data[i+\u001b[32m1\u001b[39m:i+block_size+\u001b[32m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m ix]).to(device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/vae/lib/python3.12/site-packages/torch/__init__.py:2688\u001b[39m, in \u001b[36m__getattr__\u001b[39m\u001b[34m(name)\u001b[39m\n\u001b[32m   2685\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m _lazy_modules:\n\u001b[32m   2686\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib.import_module(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, \u001b[34m__name__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2688\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmodule \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: module 'torch' has no attribute 'randit'"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835d291d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (your_env_name)",
   "language": "python",
   "name": "your_env_name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
