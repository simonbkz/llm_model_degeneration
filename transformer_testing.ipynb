{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64b1b037",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f375e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tiny corpus\n",
    "# text = (\"hello world. \\n\"\n",
    "#         \"hello transformer. \\n\"\n",
    "#         \"attention is all you need. \\n\"\n",
    "#         \"hello attention. \\n\"\n",
    "#         \"transformers generate text. \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "06c6c3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = (\"sawubona mfo. \\n\"\n",
    "        \"sawubona transformer. \\n\"\n",
    "        \"sifuna i attention. \\n\"\n",
    "        \"sawubona attention. \\n\"\n",
    "        \"transformers zikhiqiza umbhalo. \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "160f71ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build vocab (characters)\n",
    "chars = sorted(list(set(text)))\n",
    "enum_chars = {ch:i for i, ch in enumerate(chars)} #to mimic transformers tokenizer, we will need to tokenize subwords later\n",
    "item_chars = {i:ch for ch,i in enum_chars.items()}\n",
    "#vocab size from the corpus\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a2714aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(s):\n",
    "    return torch.tensor([enum_chars[c] for c in s], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0cbc4b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(ids):\n",
    "    return \"\".join([item_chars[i] for i in ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e9aa236f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f3d1b83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model = 64, nhead = 8, num_layers = 2, dim_feedforward = 128, block_size = 128, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_embedding = nn.Embedding(block_size, d_model)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward,dropout=dropout,activation='gelu',batch_first=True)\n",
    "        self.tr = nn.TransformerEncoder(encoder_layer, num_layers = num_layers)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        B, T = idx.shape\n",
    "        if T > self.block_size:\n",
    "            idx = idx[:, -self.block_size:]\n",
    "            T = self.block_size\n",
    "\n",
    "        position_ids = torch.arange(T, device = idx.device).unsqueeze(0).expand(B,T)\n",
    "        x = self.token_embedding(idx) + self.positional_embedding(position_ids)\n",
    "\n",
    "    # prevent looking ahead\n",
    "        attn_mask = torch.triu(torch.ones(T,T, device=idx.device),diagonal=1).bool()\n",
    "        x = self.tr(x, mask = attn_mask)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4d0d76fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(model, start_text, max_new_chars=200, temperature = 1.0, top_k = None, device = 'cpu'):\n",
    "    model.eval()\n",
    "    idx = encode(start_text).unsqueeze(0).to(device)\n",
    "\n",
    "    for _ in range(max_new_chars):\n",
    "        logits = model(idx)[:,-1,:] / max(temperature, 1e-6)\n",
    "        if top_k is not None:\n",
    "            v, ix = torch.topk(logits, k = top_k, dim = 1)\n",
    "            mask = torch.full_like(logits, float(\"-inf\"))\n",
    "            mask.scatter_(1, ix, v)\n",
    "            logits = mask\n",
    "\n",
    "        probs = F.softmax(logits, dim = 1)\n",
    "        next_id = torch.multinomial(probs, num_samples=1)\n",
    "        idx = torch.cat([idx, next_id], dim = 1)\n",
    "\n",
    "    return decode(idx.squeeze(0).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a2c65c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training loop \n",
    "def get_batch(data, batch_size = 32, block_size = 64, device = 'cpu'):\n",
    "    ix = torch.randint(0, len(data) - block_size -1, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix]).to(device)\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]).to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b3d06057",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    model = TinyGPT(vocab_size, nhead = 4, num_layers = 2, dim_feedforward = 128, block_size = 128).to(device)\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr = 3e-4)\n",
    "\n",
    "    steps = 1200\n",
    "    batch_size = 64\n",
    "    block_size = 64\n",
    "\n",
    "    model.train()\n",
    "    for step in range(1, steps + 1):\n",
    "        x, y = get_batch(input_data, batch_size, block_size, device = device)\n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits.reshape(-1, vocab_size), y.reshape(-1))\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        if step % 200 == 0:\n",
    "            ppl = math.exp(loss.item())\n",
    "            print(f\"step {step:4d} | loss {loss.item():.3f} | ppl {ppl:.2f}\")\n",
    "\n",
    "    #generate sample text\n",
    "    print(\"\\n--- Generated text ---\")\n",
    "    for prompt in [\"sawubona\", \"zikhiqiza\", \"transform\"]:\n",
    "        out = generate(model, prompt, max_new_chars = 120, temperature = 0.9, top_k = 10, device = device)\n",
    "        print(f\"\\nPrompt: {prompt!r}\\n{out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3d47a8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  200 | loss 0.953 | ppl 2.59\n",
      "step  400 | loss 0.298 | ppl 1.35\n",
      "step  600 | loss 0.119 | ppl 1.13\n",
      "step  800 | loss 0.078 | ppl 1.08\n",
      "step 1000 | loss 0.052 | ppl 1.05\n",
      "step 1200 | loss 0.043 | ppl 1.04\n",
      "\n",
      "--- Generated text ---\n",
      "\n",
      "Prompt: 'sawubona'\n",
      "sawubona transformer. \n",
      "sifuna i attention. \n",
      "sawubona attention. \n",
      "a \n",
      "tion. ntention. \n",
      "trantrmention. \n",
      "tionsawuwubonsa zikhiontrsa\n",
      "\n",
      "Prompt: 'zikhiqiza'\n",
      "zikhiqiza attention. \n",
      "sawubona attention. \n",
      "transformers zikhizalosa ubon. umena a \n",
      "saunsawubontionsfumikha \n",
      "santr. \n",
      "tiqizifurs a \n",
      "\n",
      "Prompt: 'transform'\n",
      "transformer. \n",
      "sifuna i attention. \n",
      "sawubona attention. \n",
      "transformens \n",
      "sformensa an. ziontionansansfomersfon. an. antionsanansfons\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835d291d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (your_env_name)",
   "language": "python",
   "name": "your_env_name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
